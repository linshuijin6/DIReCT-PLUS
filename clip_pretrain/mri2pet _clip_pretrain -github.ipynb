{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f6d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/liutuo/.conda/envs/xiaochou/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to CUDA device: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import monai.transforms as mt\n",
    "import monai.data as md\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from liutuo_utils import compare_3d_jet,compare_3d,donkey_noise_like\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "device_id =3\n",
    "\n",
    "# 设置 PyTorch 的默认 CUDA 设备\n",
    "torch.cuda.set_device(device_id)\n",
    "\n",
    "# 确认当前默认 CUDA 设备\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Switched to CUDA device: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195135f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# 使用本地路径加载处理器和模型\n",
    "local_model_path = \"/home/data/liutuo/code/BiomedCLIP\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(local_model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ab189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: torch.Size([2, 512])\n",
      "Text features dtype: torch.float32\n",
      "FDG text feature norm: 3.896071195602417\n",
      "AV45 text feature norm: 4.151429176330566\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "# 本地模型路径\n",
    "local_model_path = \"/home/data/liutuo/code/BiomedCLIP\"\n",
    "\n",
    "# 加载处理器和模型\n",
    "processor = AutoProcessor.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "# 移动到设备\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 定义文本描述\n",
    "fdg_text_optimized = (\n",
    "    \"FDG PET is a functional brain imaging technique that visualizes the dynamic changes in glucose metabolism, \"\n",
    "    \"directly linked to neuronal energy demands and synaptic activity. It serves as a tool to assess functional \"\n",
    "    \"connectivity and energy utilization across brain regions. Areas with decreased metabolic activity, such as \"\n",
    "    \"those affected by neurodegenerative diseases, should exhibit reduced signal intensity. High-intensity metabolic \"\n",
    "    \"hotspots in gray matter (e.g., the cerebral cortex and basal ganglia) are key markers of neuronal activity.\"\n",
    ")\n",
    "\n",
    "av45_text_optimized = (\n",
    "    \"AV45 PET is a molecular imaging technique that highlights the static distribution of amyloid-beta plaques, \"\n",
    "    \"a critical pathological marker of Alzheimer's disease. This imaging modality provides a spatial map of amyloid \"\n",
    "    \"deposition in cortical regions (e.g., the temporal, parietal, and frontal lobes) and can distinguish amyloid-positive \"\n",
    "    \"areas from amyloid-negative white matter regions. The primary focus is on identifying amyloid deposition patterns \"\n",
    "    \"to assess disease progression and pathological burden.\"\n",
    ")\n",
    "\n",
    "# 将两个文本放入列表\n",
    "texts = [fdg_text_optimized, av45_text_optimized]\n",
    "\n",
    "# 使用 processor/tokenizer 处理文本\n",
    "# 注意：AutoProcessor 可能直接提供 __call__，但 BiomedCLIP 通常需使用 tokenizer\n",
    "# 如果 processor 有 tokenizer 属性，用它；否则直接用 processor\n",
    "if hasattr(processor, 'tokenizer'):\n",
    "    inputs = processor.tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256  # BiomedCLIP 常用 context_length=256\n",
    "    )\n",
    "else:\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# 移动到设备\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs.get('attention_mask', None)\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "# 获取文本嵌入\n",
    "with torch.no_grad():\n",
    "    # 方法 1：尝试使用 get_text_features（适用于 transformers.CLIP-like 接口）\n",
    "    try:\n",
    "        text_features = model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    except AttributeError:\n",
    "        # 方法 2：如果失败，尝试使用 encode_text（OpenCLIP 风格）\n",
    "        try:\n",
    "            text_features = model.encode_text(input_ids)\n",
    "        except AttributeError:\n",
    "            # 方法 3：手动前向传播，取最后一层输出并归一化（常见于 CLIP）\n",
    "            outputs = model.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output  # 或 outputs.last_hidden_state[:, 0]\n",
    "            text_features = model.text_projection(pooled_output) if hasattr(model, 'text_projection') else pooled_output\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Text features shape:\", text_features.shape)  # 应为 [2, D]，例如 [2, 512]\n",
    "print(\"Text features dtype:\", text_features.dtype)\n",
    "\n",
    "# 可选：分离两个向量\n",
    "fdg_text_feature = text_features[0]      # shape: [D]\n",
    "av45_text_feature = text_features[1]     # shape: [D]\n",
    "\n",
    "print(\"FDG text feature norm:\", fdg_text_feature.norm().item())\n",
    "print(\"AV45 text feature norm:\", av45_text_feature.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca753d0b-11e8-43eb-be76-3ed138e883d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# 数据目录（保持不变）\n",
    "base_dir = \"/home/ssddata/liutuo/liutuo_data/test1\"\n",
    "mri_dir = \"/home/ssddata/liutuo/liutuo_data/test1/MRI\"\n",
    "av45_dir = \"/home/ssddata/liutuo/liutuo_data/test1/AV45\"\n",
    "fdg_dir = \"/home/ssddata/liutuo/liutuo_data/test1/FDG\"\n",
    "csv_path = os.path.join(base_dir, \"test_filtered_subjects_with_description.csv\")\n",
    "\n",
    "# JSON 文件保存路径（保持不变）\n",
    "train_json_path = \"/home/ssddata/liutuo/liutuo_data/test1/train_data_with_description.json\"\n",
    "val_json_path = \"/home/ssddata/liutuo/liutuo_data/test1/val_data_with_description.json\"\n",
    "\n",
    "# 加载 CSV 文件（保持不变）\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "csv_dict = csv_data.set_index(\"Subject ID\")[\"Description\"].to_dict()\n",
    "\n",
    "# 获取文件列表（保持不变）\n",
    "mri_files = sorted(os.listdir(mri_dir))\n",
    "av45_files = sorted(os.listdir(av45_dir))\n",
    "fdg_files = sorted(os.listdir(fdg_dir))\n",
    "\n",
    "def get_subject_id(filename):\n",
    "    \"\"\"从文件名中提取统一的 subject ID（前三个部分，如 '002_S_0295'）\"\"\"\n",
    "    parts = filename.split('_')\n",
    "    return f\"{parts[0]}_{parts[1]}_{parts[2]}\"\n",
    "\n",
    "# 使用统一的 get_subject_id 处理所有文件（保持不变）\n",
    "mri_dict = {get_subject_id(f): os.path.join(mri_dir, f) for f in mri_files}\n",
    "av45_dict = {get_subject_id(f): os.path.join(av45_dir, f) for f in av45_files}\n",
    "fdg_dict = {get_subject_id(f): os.path.join(fdg_dir, f) for f in fdg_files}\n",
    "\n",
    "# 匹配文件并加入描述信息和 Subject ID\n",
    "paired_data = []\n",
    "for patient_id, mri_file in mri_dict.items():\n",
    "    if patient_id in av45_dict and patient_id in fdg_dict:\n",
    "        # 检查描述信息是否存在\n",
    "        description = csv_dict.get(patient_id, None)  # 从 csv_dict 中获取 Description 信息\n",
    "        # 构建数据条目\n",
    "        paired_data.append({\n",
    "            \"name\": patient_id,  # 添加 name 字段\n",
    "            \"mri\": os.path.join(mri_dir, mri_file),\n",
    "            \"av45\": os.path.join(av45_dir, av45_dict[patient_id]),\n",
    "            \"fdg\": os.path.join(fdg_dir, fdg_dict[patient_id]),\n",
    "            \"description\": description  # 加入 Description 信息\n",
    "        })\n",
    "\n",
    "print(f\"Total matched pairs with description: {len(paired_data)}\")\n",
    "# 在 paired_data 中新增键 fdg_index 和 av45_index\n",
    "for idx, data in enumerate(paired_data):\n",
    "    data[\"fdg_index\"] = idx  # 将样本在 paired_data 中的索引作为 fdg_index\n",
    "    data[\"av45_index\"] = idx\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 示例：假设已经加载 BiomedCLIP 模型和处理器\n",
    "# 这里替换为您实际使用的 BiomedCLIP 模型和 tokenizer\n",
    "local_model_path = \"/home/ssddata/liutuo/BiomedCLIP\"\n",
    "processor = AutoProcessor.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 假设 paired_data 已经加载\n",
    "modal_information = [data[\"description\"] for data in paired_data if data[\"description\"] is not None]\n",
    "\n",
    "# 对描述信息进行 Tokenizer 编码\n",
    "text_inputs = processor(\n",
    "    modal_information,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256 # 根据模型输入限制选择合适的 max_length\n",
    ").to(device)\n",
    "\n",
    "# 转化为 BiomedCLIP 嵌入\n",
    "with torch.no_grad():\n",
    "    desc_text_features = model.get_text_features(text_inputs['input_ids'])  # 使用模型方法提取嵌入\n",
    "    print(f\"Shape of features: {desc_text_features.shape}, Dtype: {desc_text_features.dtype}\")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_data, val_data = train_test_split(paired_data, test_size=1, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# 保存到 JSON 文件\n",
    "with open(train_json_path, \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "\n",
    "with open(val_json_path, \"w\") as f:\n",
    "    json.dump(val_data, f, indent=4)\n",
    "\n",
    "print(f\"Saved train data to: {train_json_path}\")\n",
    "print(f\"Saved validation data to: {val_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bd0f5-20be-4da3-81f9-fa880a6885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证训练集\n",
    "with open(train_json_path, \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "print(f\"Loaded {len(train_data)} training samples.\")\n",
    "print(\"First training sample:\", train_data[0])\n",
    "\n",
    "# 验证验证集\n",
    "with open(val_json_path, \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "print(f\"Loaded {len(val_data)} validation samples.\")\n",
    "print(\"First validation sample:\", val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02699dc3-60f1-41b6-b248-2dfa559c7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, LoadImaged, Lambdad, EnsureChannelFirstd, Orientationd, CropForegroundd, \\\n",
    "    HistogramNormalized, ResizeWithPadOrCropd, Spacingd, NormalizeIntensityd, ScaleIntensityd\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import monai.transforms as mt\n",
    "\n",
    "# 定义 transform 函数\n",
    "def fdg_index_transform(x):\n",
    "    return torch.cat([desc_text_features[x].unsqueeze(0), fdg_feature_optimized], dim=0)\n",
    "\n",
    "def av45_index_transform(x):\n",
    "    return torch.cat([desc_text_features[x].unsqueeze(0), av45_feature_optimized], dim=0)\n",
    "\n",
    "\n",
    "# 自定义加载函数\n",
    "def mat_load(filepath):\n",
    "    \"\"\"\n",
    "    使用 nibabel 加载 NIfTI 文件，并转换为 NumPy 数组。\n",
    "    \"\"\"\n",
    "    return nib.load(filepath).get_fdata()\n",
    "\n",
    "# 加载 JSON 文件\n",
    "train_json_path = \"/home/ssddata/liutuo/liutuo_data/test1/train_data_with_description.json\"\n",
    "val_json_path = \"/home/ssddata/liutuo/liutuo_data/test1/val_data_with_description.json\"\n",
    "\n",
    "# 加载 JSON 文件\n",
    "with open(train_json_path, \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(val_json_path, \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# 转换数据格式\n",
    "train_data = [\n",
    "    {\n",
    "        \"name\": item[\"name\"],\n",
    "        \"mri\": item[\"mri\"],\n",
    "        \"av45\": item[\"av45\"],\n",
    "        \"fdg\": item[\"fdg\"],\n",
    "        \"description\": item.get(\"description\", {}),  # 确保 description 存在\n",
    "        \"fdg_index\": item.get(\"fdg_index\"),  # 保留 fdg_index\n",
    "        \"av45_index\": item.get(\"av45_index\"),  # 保留 av45_index\n",
    "    }\n",
    "    for item in train_data\n",
    "]\n",
    "\n",
    "val_data = [\n",
    "    {\n",
    "        \"name\": item[\"name\"],\n",
    "        \"mri\": item[\"mri\"],\n",
    "        \"av45\": item[\"av45\"],\n",
    "        \"fdg\": item[\"fdg\"],\n",
    "        \"description\": item.get(\"description\", {}),  # 确保 description 存在\n",
    "        \"fdg_index\": item.get(\"fdg_index\"),  # 保留 fdg_index\n",
    "        \"av45_index\": item.get(\"av45_index\"),  # 保留 av45_index\n",
    "    }\n",
    "    for item in val_data\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# 构建数据增强 pipeline\n",
    "# 定义训练集数据增强流程\n",
    "train_transforms = mt.Compose([\n",
    "    mt.Lambdad(keys=[\"mri\", \"fdg\",\"av45\"], func=mat_load),  # 加载 NIfTI 文件\n",
    "    mt.EnsureChannelFirstd(keys=[\"mri\", \"fdg\",\"av45\"],channel_dim='no_channel'),\n",
    "    mt.Orientationd(keys=[\"mri\", \"fdg\",\"av45\"], axcodes=\"LPI\"),\n",
    "    mt.CropForegroundd(keys=[\"mri\", \"fdg\",\"av45\"], source_key=\"mri\"),\n",
    "    mt.HistogramNormalized(keys=[\"mri\"]),\n",
    "    mt.ResizeWithPadOrCropd(keys=[\"mri\", \"fdg\",\"av45\"], spatial_size=[160, 192, 160]),\n",
    "    mt.Spacingd(keys=[\"mri\", \"fdg\",\"av45\"], pixdim=(1.0, 1.0, 1.0)),\n",
    "    mt.NormalizeIntensityd(keys=[\"mri\", \"fdg\",\"av45\"]),\n",
    "    mt.ScaleIntensityd(keys=[\"mri\", \"fdg\",\"av45\"]),\n",
    "    # mt.Lambdad(keys=[\"fdg_index\"], func=fdg_index_transform),  # 添加 fdg_index 转换\n",
    "    # mt.Lambdad(keys=[\"av45_index\"], func=av45_index_transform),  # 添加 av45_index 转换\n",
    "])\n",
    "\n",
    "# 定义验证集数据增强流程（通常与训练集一致，但不含随机性增强）\n",
    "val_transforms = mt.Compose([\n",
    "    mt.Lambdad(keys=[\"mri\", \"fdg\",\"av45\"], func=mat_load),  # 加载 NIfTI 文件\n",
    "    mt.EnsureChannelFirstd(keys=[\"mri\", \"fdg\",\"av45\"],channel_dim='no_channel'),\n",
    "    mt.Orientationd(keys=[\"mri\", \"fdg\",\"av45\"], axcodes=\"LPI\"),\n",
    "    mt.CropForegroundd(keys=[\"mri\", \"fdg\",\"av45\"], source_key=\"mri\"),\n",
    "    mt.HistogramNormalized(keys=[\"mri\"]),\n",
    "    mt.ResizeWithPadOrCropd(keys=[\"mri\", \"fdg\",\"av45\"], spatial_size=[160, 192, 160]),\n",
    "    mt.Spacingd(keys=[\"mri\", \"fdg\",\"av45\"], pixdim=(1.0, 1.0, 1.0)),\n",
    "    mt.NormalizeIntensityd(keys=[\"mri\", \"fdg\",\"av45\"]),\n",
    "    mt.ScaleIntensityd(keys=[\"mri\", \"fdg\",\"av45\"]),\n",
    "    # mt.Lambdad(keys=[\"fdg_index\"], func=fdg_index_transform),  # 添加 fdg_index 转换\n",
    "    # mt.Lambdad(keys=[\"av45_index\"], func=av45_index_transform),  # 添加 av45_index 转换\n",
    "])\n",
    "\n",
    "\n",
    "# 构建 CacheDataset\n",
    "train_ds = CacheDataset(data=train_data, transform=train_transforms, num_workers=0, )\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)    #shuffle=True\n",
    "\n",
    "val_ds = CacheDataset(data=val_data, transform=val_transforms, num_workers=0, )\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# 测试 DataLoader\n",
    "for batch_data in train_loader:\n",
    "    print(\"MRI shape:\", batch_data[\"mri\"].shape)\n",
    "    print(\"AV45 shape:\", batch_data[\"av45\"].shape)\n",
    "    print(\"FDG shape:\", batch_data[\"fdg\"].shape)\n",
    "    print(\"description:\", train_data[0][\"description\"])  # 从原始数据访问描述信息\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce342de-c52d-4e40-809f-7ea3b9661a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    image_mri =batch[\"mri\"].to(device)\n",
    "    seg_fdg = batch[\"fdg\"].to(device) # this is the ground truth segmentation\n",
    "    seg_av45 = batch[\"av45\"].to(device)\n",
    "    # fdg_index=batch[\"fdg_index\"].to(device)\n",
    "    # av45_index=batch[\"av45_index\"].to(device)\n",
    "    names = batch[\"name\"]  # Extract subject information\n",
    "\n",
    "   \n",
    "    for idx, name in enumerate(names):\n",
    "         print(f\"Subject name: {name}\")\n",
    "    compare_3d([image_mri, seg_fdg,seg_av45])\n",
    "    break  # Uncomment to compare only the first batch , label_1,label_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5894e-7af9-4826-b8f8-6d7adba99022",
   "metadata": {},
   "source": [
    "#obtain fdg_pet_vectors and av45_pet_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a807b1d-110c-424a-87b2-07322c4ac49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image  # 导入 PIL.Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "# 1. 加载 BiomedCLIP 模型和处理器\n",
    "local_model_path = \"/home/data/liutuo/code/BiomedCLIP\"\n",
    "processor = AutoProcessor.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. 定义 BiomedCLIP 特征提取函数\n",
    "def extract_pet_features_from_monai(batch, num_slices=160):\n",
    "\n",
    "    pet_data = batch[\"fdg\"]  # 提取 PET 数据 (batch_size, 1, 160, 192, 160)\n",
    "    pet_data = pet_data.squeeze(0).squeeze(0).numpy()  # 移除 batch 和通道维度 -> (160, 192, 160)\n",
    "    \n",
    "    # 提取 num_slices 张切片（均匀采样）\n",
    "    slice_indices = np.linspace(0, pet_data.shape[2] - 1, num_slices, dtype=int)\n",
    "    slices = [pet_data[:, :, i] for i in slice_indices]\n",
    "    \n",
    "    # 预处理切片 -> 224x224 RGB\n",
    "    processed_slices = []\n",
    "    for slice_2d in slices:\n",
    "        slice_min, slice_max = slice_2d.min(), slice_2d.max()\n",
    "        normalized_slice = (slice_2d - slice_min) / (slice_max - slice_min + 1e-5)\n",
    "        img = Image.fromarray((normalized_slice * 255).astype(np.uint8)).resize((224, 224))\n",
    "        img_rgb = np.stack([img] * 3, axis=-1)\n",
    "        processed_slices.append(Image.fromarray(img_rgb.astype(np.uint8)))\n",
    "    \n",
    "    # 使用 BiomedCLIP 提取切片特征\n",
    "    inputs = processor(images=processed_slices, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        slice_features = model.get_image_features(**inputs)  # (num_slices, 512)\n",
    "    \n",
    "    # 平均池化得到全局特征\n",
    "    global_feature = torch.mean(slice_features, dim=0)  # (512,)\n",
    "    return global_feature.cpu().detach()\n",
    "\n",
    "# 3. 遍历 train_loader，提取所有样本的特征\n",
    "pet_vectors = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Processing batch {i+1}/{len(train_loader)}...\")\n",
    "    pet_vector = extract_pet_features_from_monai(batch)  # 提取单张 PET 图像的特征\n",
    "    pet_vectors.append(pet_vector)\n",
    "\n",
    "# 堆叠所有特征向量\n",
    "pet_vectors = torch.stack(pet_vectors)  # Shape: (1129, 512)\n",
    "\n",
    "# 4. 保存所有特征\n",
    "\n",
    "output_path = \"/home/data/liutuo/code/checkpoint/fdg_pet_vectors_from_monai.pt\"\n",
    "torch.save(pet_vectors, output_path)\n",
    "print(f\"所有 PET 特征向量已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4c46b8-8d37-4523-874f-bbf738111e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=-0.5203, FDG_sim=0.3452, AV45_sim=0.3205, Cross_sim=0.7819\n",
      "\n",
      "✅ Optimization Finished!\n",
      "Final FDG-text → FDG-images similarity: 0.9745\n",
      "Final AV45-text → AV45-images similarity: 0.9714\n",
      "Final FDG-Adapt ↔ AV45-Adapt similarity: 0.8241\n",
      "Learned a=0.7526, c=0.7437\n",
      "\n",
      "Learned b: norm = 0.9282, mean = -0.0010\n",
      "Learned d: norm = 0.9356, mean = 0.0018\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载图像特征（假设已归一化；若未归一化，后续会处理）\n",
    "fdg_img_features = torch.load(\"/home/data/liutuo/code/checkpoint/fdg_pet_vectors_from_monai.pt\").to(device)  # [1598, 512]\n",
    "av45_img_features = torch.load(\"/home/data/liutuo/code/checkpoint/av45_pet_vectors_from_monai.pt\").to(device)  # [1365, 512]\n",
    "\n",
    "# 归一化图像特征（确保是单位向量）\n",
    "fdg_img_features = F.normalize(fdg_img_features, dim=1)\n",
    "av45_img_features = F.normalize(av45_img_features, dim=1)\n",
    "\n",
    "\n",
    "fdg_text_feature = fdg_text_feature.to(device)  # shape [512]\n",
    "av45_text_feature = av45_text_feature.to(device)  # shape [512]\n",
    "\n",
    "# 确保是 unit vector（L2 norm = 1）\n",
    "fdg_text_feature = F.normalize(fdg_text_feature, dim=-1)\n",
    "av45_text_feature = F.normalize(av45_text_feature, dim=-1)\n",
    "\n",
    "\n",
    "# 3. 初始化可学习参数（修正版）\n",
    "# ----------------------------\n",
    "a = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "c = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "b = torch.randn(512, device=device) * 0.01\n",
    "b.requires_grad_(True)\n",
    "\n",
    "d = torch.randn(512, device=device) * 0.01\n",
    "d.requires_grad_(True)\n",
    "\n",
    "params = [a, b, c, d]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "# 目标文本-图像相似度（越高越好 → 负相似度作为 loss）\n",
    "# 目标 fdg-av45 相似度\n",
    "target_cross_sim = 0.82\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 训练循环\n",
    "# ----------------------------\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 构建 adapt 向量\n",
    "    adapt_fdg = a * fdg_text_feature + b  # [512]\n",
    "    adapt_av45 = c * av45_text_feature + d  # [512]\n",
    "\n",
    "    # 归一化（用于余弦相似度）\n",
    "    adapt_fdg_norm = F.normalize(adapt_fdg, dim=-1)\n",
    "    adapt_av45_norm = F.normalize(adapt_av45, dim=-1)\n",
    "\n",
    "    # Loss 1: maximize mean cosine similarity with FDG image features\n",
    "    sim_fdg = F.cosine_similarity(adapt_fdg_norm.unsqueeze(0), fdg_img_features, dim=1)  # [1598]\n",
    "    loss_fdg = -sim_fdg.mean()  # 负号：最大化 → 最小化负值\n",
    "\n",
    "    # Loss 2: maximize mean cosine similarity with AV45 image features\n",
    "    sim_av45 = F.cosine_similarity(adapt_av45_norm.unsqueeze(0), av45_img_features, dim=1)  # [N]\n",
    "    loss_av45 = -sim_av45.mean()\n",
    "\n",
    "    # Loss 3: make cosine similarity between adapt_fdg and adapt_av45 close to 0.82\n",
    "    cross_sim = F.cosine_similarity(adapt_fdg_norm.unsqueeze(0), adapt_av45_norm.unsqueeze(0))\n",
    "    loss_cross = (cross_sim - target_cross_sim) ** 2\n",
    "\n",
    "    # 总损失（可调整权重）\n",
    "    total_loss = loss_fdg + loss_av45 + 100.0 * loss_cross  # 提高 cross 约束权重\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5000== 0:\n",
    "        print(f\"Epoch {epoch}: Loss={total_loss.item():.4f}, \"\n",
    "              f\"FDG_sim={-loss_fdg.item():.4f}, \"\n",
    "              f\"AV45_sim={-loss_av45.item():.4f}, \"\n",
    "              f\"Cross_sim={cross_sim.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 获取最终结果\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    final_adapt_fdg = F.normalize(a * fdg_text_feature + b, dim=-1)\n",
    "    final_adapt_av45 = F.normalize(c * av45_text_feature + d, dim=-1)\n",
    "    final_cross_sim = F.cosine_similarity(final_adapt_fdg.unsqueeze(0), final_adapt_av45.unsqueeze(0))\n",
    "    fdg_img_sim = F.cosine_similarity(final_adapt_fdg.unsqueeze(0), fdg_img_features).mean()\n",
    "    av45_img_sim = F.cosine_similarity(final_adapt_av45.unsqueeze(0), av45_img_features).mean()\n",
    "\n",
    "print(\"\\n✅ Optimization Finished!\")\n",
    "print(f\"Final FDG-text → FDG-images similarity: {fdg_img_sim.item():.4f}\")\n",
    "print(f\"Final AV45-text → AV45-images similarity: {av45_img_sim.item():.4f}\")\n",
    "print(f\"Final FDG-Adapt ↔ AV45-Adapt similarity: {final_cross_sim.item():.4f}\")\n",
    "print(f\"Learned a={a.item():.4f}, c={c.item():.4f}\")\n",
    "# 打印 b, d 的统计信息\n",
    "print(f\"\\nLearned b: norm = {b.norm().item():.4f}, mean = {b.mean().item():.4f}\")\n",
    "print(f\"Learned d: norm = {d.norm().item():.4f}, mean = {d.mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaochou",
   "language": "python",
   "name": "xiaochou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
